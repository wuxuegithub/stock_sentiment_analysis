{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install twython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dir_name= os.getcwd()+'/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = \"sP6nUpJCbDi1e7ceEyt5H2Ifk\" \n",
    "consumer_secret = \"fr1AWhS8YTHnzwzmbuDvgzRauLu5L4Hoyjd5y9RMJhztEQLfqT\" \n",
    "access_token = \"1303523786793168896-FCnTCWSAkHIKdnMaAaASuHMbCr0PTn\" \n",
    "access_token_secret = \"yqtpi6yDzsnC1rMzZArkBfYq654DblYqMph6xiL3pNdvW\"\n",
    "\n",
    "\n",
    "# authorization of consumer key and consumer secret \n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "dir_name= os.getcwd()+'/data/'\n",
    "\n",
    "nasdaq_files=os.path.join(dir_name,'stock_ticker_datasets/nasdaq.csv')\n",
    "nyse_files=os.path.join(dir_name,'stock_ticker_datasets/nyse.csv')\n",
    "\n",
    "\n",
    "nasdaq=pd.read_csv(nasdaq_files) \n",
    "nyse=pd.read_csv(nyse_files)\n",
    "\n",
    "nasdaq['Symbol']=nasdaq['Symbol'].astype(str)\n",
    "nasdaq_input=nasdaq['Symbol']\n",
    "\n",
    "\n",
    "nyse['Symbol']=nyse['Symbol'].astype(str)\n",
    "nyse_input=nyse['Symbol']\n",
    "# 3081\n",
    "n = 100  #chunk row size\n",
    "nasdaq_df = [nasdaq_input[i:i+n] for i in range(3546,nasdaq_input.shape[0],n)]\n",
    "nyse_df = [nyse_input[i:i+n] for i in range(0,nyse_input.shape[0],n)]\n",
    "\n",
    "\n",
    "nasdaq.set_index(\"Symbol\" , inplace=True)\n",
    "nyse.set_index(\"Symbol\" , inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time\n",
    "import csv\n",
    "def returnTimelinetweets(name,number_of_tweets):\n",
    "# # get data from the company's twitter timeline\n",
    "    alltweets = []\n",
    "    user=api.get_user(name)\n",
    "    userid=user.id_str\n",
    "        #extract data from \n",
    "    timeline = api.user_timeline(user_id=userid,count=number_of_tweets)\n",
    "    alltweets.extend(timeline)\n",
    "    oldest=alltweets[-1].id-1\n",
    "#         outtweets=[[tweet.user.screen_name,tweet.created_at,tweet.text.encode(\"utf-8\")]for tweet in alltweets if]\n",
    "    outtweets=[]\n",
    "    for tweet in alltweets:\n",
    "#             if(datetime.datetime.now()-tweet.created_at).days<1:\n",
    "        outtweets=[tweet.created_at,tweet.text.encode(\"utf-8\")]\n",
    "\n",
    "    path=os.path.join(dir_name,'data-tweets/')\n",
    "    with open(path+'data-'+name+'-tweets.csv','a') as f:\n",
    "        writer= csv.writer(f)\n",
    "        writer.writerows(outtweets)\n",
    "            \n",
    "# get data from  the cashtag\n",
    "def get_tagtweets(name,day):\n",
    "    allhashtag=[]\n",
    "    print(name)\n",
    "    hashtags=tweepy.Cursor(api.search,q=name,lang='en',tweet_mode='extended').items(200)\n",
    "    outttags=[] \n",
    "    path=os.path.join(dir_name,'data-tweets/'+'data-'+name+'-tweets.csv')\n",
    "    print(path)\n",
    "    try:\n",
    "        with open(path,'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for status in hashtags:\n",
    "                if(datetime.datetime.now()-status.created_at).days<=day:\n",
    "        #                     tweet_text = status.full_text.encode(\"utf-8\")\n",
    "                    tweet_text = status.full_text\n",
    "                    dates=str(status.created_at)[:10]\n",
    "                    print(dates)\n",
    "                    print(tweet_text)\n",
    "                    writer.writerow([dates,tweet_text])\n",
    "        \n",
    "            sleep(2000)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "    \n",
    "# get_tagtweets('AAPL',9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection using tweepy stream API by checking for containing the company hashtag \n",
    "import tweepy, datetime, time\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "# file=[]\n",
    "\n",
    "\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self, output_file=sys.stdout,input_name=sys.stdout):\n",
    "        super(MyStreamListener,self).__init__()\n",
    "        self.output_file = output_file\n",
    "        self.max_tweets=200\n",
    "        self.tweet_count=0\n",
    "        self.input_name=input_name\n",
    "        \n",
    "        \n",
    "    def on_status(self, status):\n",
    "        if(self.tweet_count==self.max_tweets):\n",
    "            return False\n",
    "        elif(datetime.datetime.now()-status.created_at).days<1:\n",
    "            tweet_text = status.text\n",
    "            print(tweet_text)\n",
    "            writer = csv.writer(self.output_file)\n",
    "            \n",
    "            writer.writerow([status.created_at,status.extended_tweet['full_text'].encode(\"utf-8\")])\n",
    "            self.tweet_count+=1\n",
    "            \n",
    "        \n",
    "    def on_error(self, status_code):\n",
    "        print(status_code)\n",
    "        if status_code == 420:\n",
    "            #returning False in on_error disconnects the stream\n",
    "            \n",
    "            return False\n",
    "        \n",
    "    def on_timeout(self):\n",
    "        sys.stderr.write(\"Timeout, sleeping for 60 seconds...\\n\")\n",
    "        time.sleep(60)\n",
    "        return \n",
    "            \n",
    "\n",
    "# def get_tweets(tweet):\n",
    "#     print (tweet.text.encode(\"utf-8\"))\n",
    "    \n",
    "# list=['$AAPL','$AACG']\n",
    "# for firm in list:\n",
    "\n",
    "#         path=os.path.join(dir_name,'data-tweets/')\n",
    "#         with open(path+'data-'+name+'-tweets.csv','a') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         myStreamListener = MyStreamListener(output_file=f,input_name=firm)\n",
    "#         myStream = tweepy.Stream(auth = api.auth, tweet_mode='extended',listener=myStreamListener,languages = [\"en\"])\n",
    "#         myStream.filter(track=firm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_tweets_all(day):\n",
    "    for list in nasdaq_df:\n",
    "        for name in list:\n",
    "            name='$'+name\n",
    "            get_tagtweets(name,day)\n",
    "    for list in nyse_df:\n",
    "        for name in list:\n",
    "            name='$'+name\n",
    "            get_tagtweets(name,day)\n",
    "            \n",
    "update_tweets_all(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_today_tweets(name,day):\n",
    "    text=[]\n",
    "    date=[]\n",
    "    tweets=pd.DataFrame()\n",
    "    \n",
    "    hashtags=tweepy.Cursor(api.search,q=name,lang='en',tweet_mode='extended').items(300)\n",
    "    outttags=[] \n",
    "    path=os.path.join(dir_name,'data-tweets/')\n",
    "    for status in hashtags:\n",
    "        if(datetime.datetime.now()-status.created_at).days<=day:\n",
    "            tweet_text = status.full_text.encode(\"utf-8\")\n",
    "            dates=str(status.created_at)[:10]\n",
    "            text.append(tweet_text)\n",
    "            dates.append(dates)\n",
    "    tweets['date']=dates\n",
    "    \n",
    "    ticker=[name for i in range(len(dates))]\n",
    "    tweets['ticker']=ticker\n",
    "    tweets['text']=text\n",
    "    \n",
    "    return tweets\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nasdaq_no_label_tweets(name):\n",
    "    df = pd.DataFrame()\n",
    "#     for nas in nyse_df:\n",
    "#          for name in nas:\n",
    "    try:\n",
    "        path=os.path.join(dir_name,'data-tweets/'+'data-$'+name+'-tweets.csv')\n",
    "        df_tweets = pd.read_csv(path ,names=['dates','tweets'],index_col='dates')\n",
    "        sector=nyse.loc[name]['Sector']\n",
    "        ticker=[name for i in range(len(df_tweets))]\n",
    "        Sector=[sector for i in range(len(df_tweets))]\n",
    "        df_tweets['ticker']=ticker\n",
    "        df_tweets['sector']=Sector\n",
    "        df=df.append(df_tweets)\n",
    "\n",
    "\n",
    "    except:\n",
    "        print('no such file exist')\n",
    "        pass\n",
    "    df_path=os.path.join(dir_name,'train-data/nasdaq/nasdaq_no_labelled_tweets.csv')\n",
    "    df.to_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nyse_no_label_tweets(name):\n",
    "    df = pd.DataFrame()\n",
    "#     for nas in nyse_df:\n",
    "#          for name in nas:\n",
    "    try:\n",
    "        path=os.path.join(dir_name,'data-tweets/'+'data-$'+name+'-tweets.csv')\n",
    "        df_tweets = pd.read_csv(path ,names=['dates','tweets'],index_col='dates')\n",
    "        sector=nyse.loc[name]['Sector']\n",
    "        ticker=[name for i in range(len(df_tweets))]\n",
    "        Sector=[sector for i in range(len(df_tweets))]\n",
    "        df_tweets['ticker']=ticker\n",
    "        df_tweets['sector']=Sector\n",
    "        df=df.append(df_tweets)\n",
    "\n",
    "\n",
    "    except:\n",
    "        print('no such file exist')\n",
    "        pass\n",
    "    df_path=os.path.join(dir_name,'train-data/nyse/nyse_no_labelled_tweets.csv')\n",
    "    df.to_csv(df_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

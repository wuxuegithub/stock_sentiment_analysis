{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep \"tensorflow\"   # Check tensorflow==2.0.0, tensorflow-gpu==2.0.0\n",
    "!pip list | grep \"transformers\" # Check transformers>=2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "dir_name= os.getcwd()+'/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification,TFBertModel\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nasdaq_files=\"/userhome/cs/wuxue/Sentiment_Analysis/stock_ticker_datasets/nasdaq.csv\"\n",
    "# nyse_files=\"/userhome/cs/wuxue/Sentiment_Analysis/stock_ticker_datasets/nyse.csv\"\n",
    "# nasdaq=pd.read_csv(nasdaq_files) \n",
    "# nyse=pd.read_csv(nyse_files)\n",
    "\n",
    "# nasdaq['Symbol']=nasdaq['Symbol'].astype(str)\n",
    "# nasdaq_input=nasdaq['Symbol']\n",
    "\n",
    "# nyse['Symbol']=nyse['Symbol'].astype(str)\n",
    "# nyse_input=nyse['Symbol']\n",
    "# # 3081\n",
    "# n = 100  #chunk row size\n",
    "# nasdaq_df = [nasdaq_input[i:i+n] for i in range(0,nasdaq_input.shape[0],n)]\n",
    "# nyse_df = [nyse_input[i:i+n] for i in range(0,nyse_input.shape[0],n)]\n",
    "# df=nasdaq_df+nyse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "# for list in nasdaq_df:\n",
    "#      for name in list:\n",
    "#             try:\n",
    "#                 df_tweets = pd.read_csv('/userhome/cs/wuxue/Sentiment_Analysis/data-tweets/data-$%s-tweets.csv'%name ,names=['dates','tweets'],index_col='dates')\n",
    "#                 df_dates=pd.read_csv('/userhome/cs/wuxue/Sentiment_Analysis/stock_label/data-%s-label.csv'%name,names=['dates','label'],index_col='dates')\n",
    "\n",
    "#                 merge=pd.merge(df_tweets,df_dates, how='inner', left_index=True, right_index=True)\n",
    "#                 df=df.append(merge)\n",
    "                \n",
    "#             except:\n",
    "#                 print('no such file exist')\n",
    "#                 pass\n",
    "# df.to_csv('labelled_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_tweets(name):\n",
    "   \n",
    "    print('reading tweets..')\n",
    "    \n",
    "    df = pd.read_csv(name)\n",
    "    print(df)\n",
    "\n",
    "    df = df.drop(['Unnamed: 0','dates','ticker','compound_vader_score'],axis=1)\n",
    "    df=df.reset_index(drop=True)\n",
    "    df=df.dropna()\n",
    "    \n",
    "    possible_labels = df.pred_label.unique()\n",
    "    label_dict = {}\n",
    "    for index, possible_label in enumerate(possible_labels):\n",
    "        label_dict[possible_label] = index\n",
    "    \n",
    "    \n",
    "    df['pred_label'] = df.pred_label.replace(label_dict)\n",
    "    x_train,y_train=train_test_split(df,test_size=0.2,random_state=0,stratify=df.pred_label.values)\n",
    "#     print(x_train)\n",
    "    return x_train,y_train\n",
    "# read_tweets('nasdaq_labelled_tweets.csv')\n",
    "\n",
    "def read_news(name):\n",
    "    print('reading news..')\n",
    "    \n",
    "    df = pd.read_csv(name)\n",
    "    print(df)\n",
    "   \n",
    "    df = df.drop(['Unnamed: 0','dates','ticker','compound_vader_score'],axis=1)\n",
    "    df=df.reset_index(drop=True)\n",
    "    df=df.dropna()\n",
    "#     label_dict = {0:1,1:2,2:3}\n",
    "    possible_labels = df.pred_label.unique()\n",
    "    label_dict = {}\n",
    "    for index, possible_label in enumerate(possible_labels):\n",
    "        label_dict[possible_label] = index\n",
    "    print(label_dict)\n",
    "\n",
    "    df['pred_label'] = df.pred_label.replace(label_dict)\n",
    "    x_train,y_train=train_test_split(df,\n",
    "                                     test_size=0.2,\n",
    "                                     random_state=0,\n",
    "                                     stratify=df.pred_label.values)\n",
    "#     print(x_train)\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN):\n",
    "    print('converting data')\n",
    "    \n",
    "    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "    return train_InputExamples, validation_InputExamples\n",
    "\n",
    "# train_InputExamples, validation_InputExamples = convert_data_to_examples(x_train,  y_train, 'tweets_x', 'tweets_y')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    print('converting examples to tf_datasets')\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "    \n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "    def gen():\n",
    "        \n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(file_name):\n",
    "    print('model training')\n",
    "    x_train,y_train=read_tweets(file_name)    \n",
    "\n",
    "    train_InputExamples, validation_InputExamples = convert_data_to_examples(x_train, y_train, 'tweets', 'pred_label')\n",
    "#     print(train_InputExamples)\n",
    "    train_data = convert_examples_to_tf_dataset(train_InputExamples.values.tolist(), tokenizer)\n",
    "    print(train_data)\n",
    "    train_data = train_data.shuffle(100).batch(15).repeat(2)\n",
    "\n",
    "\n",
    "    validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "#     validation_data = convert_examples_to_tf_dataset(validation_InputExamples.values.tolist(), tokenizer)\n",
    "    validation_data = validation_data.batch(15)\n",
    "    return train_data,validation_data\n",
    "\n",
    "def news_model_train(file_name):\n",
    "    print('model training')\n",
    "    x_train,y_train=read_news(file_name)    \n",
    "\n",
    "    train_InputExamples, validation_InputExamples = convert_data_to_examples(x_train, y_train, 'news', 'pred_label')\n",
    "#     print(train_InputExamples)\n",
    "    train_data = convert_examples_to_tf_dataset(train_InputExamples.values.tolist(), tokenizer)\n",
    "    print(train_data)\n",
    "    train_data = train_data.shuffle(100).batch(15).repeat(2)\n",
    "\n",
    "\n",
    "    validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "#     validation_data = convert_examples_to_tf_dataset(validation_InputExamples.values.tolist(), tokenizer)\n",
    "    validation_data = validation_data.batch(15)\n",
    "    return train_data,validation_data\n",
    "# model_train('nasdaq_labelled_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(train_data,validation_data):\n",
    "    print('validation..')\n",
    "#     model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "    \n",
    "\n",
    "    # model.fit(train_data, epochs=2, validation_data=validation_data)\n",
    "    history=model.fit(train_data, epochs=2, validation_data=validation_data)\n",
    "    print(history)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def accuracy_visualization(history):\n",
    "    print('visualization')\n",
    "    loss_train = history.history['accuracy']\n",
    "    loss_train = np.array(loss_train)\n",
    "    loss_val = history.history['val_accuracy']\n",
    "    loss_val=np.array(loss_val)\n",
    "    print(loss_train)\n",
    "    print(loss_val)\n",
    "    epochs=range(1, len(loss_train) + 1)\n",
    "    plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
    "    plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
    "    plt.title('Training and Validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.ylim(ymax=1)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data training\n",
    "def start(file_name):\n",
    "    train_data,validation_data=model_train(file_name)\n",
    "    history=model_validation(train_data,validation_data)\n",
    "    return history\n",
    "    \n",
    "\n",
    "def news_start(file_name):\n",
    "    train_data,validation_data=news_model_train(file_name)\n",
    "    history=model_validation(train_data,validation_data)\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nasdaq_vader_tweets_path=os.path.join(dir_name,'train-data/nasdaq/nasdaq_vader_tweets.csv')\n",
    "nasdaq_tweets_history=start(nasdaq_vader_tweets_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy_visualization(nasdaq_tweets_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nasdaq_vader_news_path=os.path.join(dir_name,'train-data/nasdaq/nasdaq_vader_news.csv')\n",
    "    \n",
    "nasdaq_news_history=news_start(nasdaq_vader_news_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_visualization(nasdaq_news_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nyse_vader_news_path=os.path.join(dir_name,'train-data/nyse/nyse_vader_news.csv')\n",
    "nyse_news_history=news_start(nyse_vader_news_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_visualization(nyse_news_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_vader_tweets_path=os.path.join(dir_name,'train-data/nyse/nyse_vader_tweets.csv')\n",
    "nyse_tweets_history=start(nyse_vader_tweets_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_visualization(nyse_tweets_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "hkex_news_history=os.path.join(dir_name,'train-data/hkex/hkex_vader_news.csv')\n",
    "hkex_news_history=news_start(hkex_news_history)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_visualization(hkex_news_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
